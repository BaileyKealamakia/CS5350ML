{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:13.446988800Z",
     "start_time": "2023-12-08T21:47:13.443971600Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "     variance  skewness  curtosis  entropy  label\n0    3.848100  10.15390  -3.85610 -4.22280      0\n1    4.004700   0.45937   1.36210  1.61810      0\n2   -0.048008  -1.60370   8.47560  0.75558      0\n3   -1.266700   2.81830  -2.42600 -1.88620      1\n4    2.203400   5.99470   0.53009  0.84998      0\n..        ...       ...       ...      ...    ...\n867  0.273310   4.87730  -4.91940 -5.81980      1\n868  1.063700   3.69570  -4.15940 -1.93790      1\n869 -1.242400  -1.71750  -0.52553 -0.21036      1\n870  1.837300   6.12920   0.84027  0.55257      0\n871 -2.014900   3.68740  -1.93850 -3.89180      1\n\n[872 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variance</th>\n      <th>skewness</th>\n      <th>curtosis</th>\n      <th>entropy</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.848100</td>\n      <td>10.15390</td>\n      <td>-3.85610</td>\n      <td>-4.22280</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.004700</td>\n      <td>0.45937</td>\n      <td>1.36210</td>\n      <td>1.61810</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.048008</td>\n      <td>-1.60370</td>\n      <td>8.47560</td>\n      <td>0.75558</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.266700</td>\n      <td>2.81830</td>\n      <td>-2.42600</td>\n      <td>-1.88620</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.203400</td>\n      <td>5.99470</td>\n      <td>0.53009</td>\n      <td>0.84998</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>867</th>\n      <td>0.273310</td>\n      <td>4.87730</td>\n      <td>-4.91940</td>\n      <td>-5.81980</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>868</th>\n      <td>1.063700</td>\n      <td>3.69570</td>\n      <td>-4.15940</td>\n      <td>-1.93790</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>869</th>\n      <td>-1.242400</td>\n      <td>-1.71750</td>\n      <td>-0.52553</td>\n      <td>-0.21036</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>870</th>\n      <td>1.837300</td>\n      <td>6.12920</td>\n      <td>0.84027</td>\n      <td>0.55257</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>-2.014900</td>\n      <td>3.68740</td>\n      <td>-1.93850</td>\n      <td>-3.89180</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>872 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in the data\n",
    "\n",
    "columns = [\n",
    "    'variance',\n",
    "    'skewness',\n",
    "    'curtosis',\n",
    "    'entropy',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "train_data = pd.read_csv('bank-note/train.csv', names=columns,header=None)\n",
    "test_data = pd.read_csv('bank-note/test.csv', names=columns, header=None)\n",
    "\n",
    "train_data\n",
    "\n",
    "#Do i need to change labels?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:14.117229300Z",
     "start_time": "2023-12-08T21:47:14.071788300Z"
    }
   },
   "id": "35120bced3703795"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:15.035863100Z",
     "start_time": "2023-12-08T21:47:15.006048400Z"
    }
   },
   "id": "338359cc6d0c9df8"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "x_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "x_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:15.597177700Z",
     "start_time": "2023-12-08T21:47:15.574842800Z"
    }
   },
   "id": "f891c07ec6cba6f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Three Layer Artificial Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70cda832bf581cc3"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:16.636205900Z",
     "start_time": "2023-12-08T21:47:16.619665Z"
    }
   },
   "id": "2ecfa76961621eed"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# the input should be segmoid_activation\n",
    "def sigmoid_activation_derivative(x):\n",
    "    return sigmoid_activation(x) * (1 - sigmoid_activation(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:47:17.117108700Z",
     "start_time": "2023-12-08T21:47:17.097629900Z"
    }
   },
   "id": "2823ec82e17e538c"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "#making a class definition to initializze easier and practice notation for pytorch\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, feature_size, hidden_size1, hidden_size2, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        self.weights_input_hidden1 = np.random.randn(self.input_size, self.hidden_size1)\n",
    "        self.weights_hidden1_hidden2 = np.random.randn(self.hidden_size1, self.hidden_size2)\n",
    "        self.weights_hidden2_output = np.random.randn(self.hidden_size2, self.output_size)\n",
    "\n",
    "        # Bias terms initialized as 1\n",
    "        self.bias_hidden1 = np.ones((1, self.hidden_size1))\n",
    "        self.bias_hidden2 = np.ones((1, self.hidden_size2))\n",
    "\n",
    "    def forward_pass(self, features):\n",
    "        # Forward pass\n",
    "        self.hidden_input1 = np.dot(features, self.weights_input_hidden1)\n",
    "        self.hidden_output1 = sigmoid_activation(self.hidden_input1)\n",
    "\n",
    "        self.hidden_input2 = np.dot(self.hidden_output1, self.weights_hidden1_hidden2)\n",
    "        self.hidden_output2 = sigmoid_activation(self.hidden_input2)\n",
    "\n",
    "        # NO ACTIVATION FOR OUTPUT NODE\n",
    "        self.output = (np.dot(self.hidden_output2, self.weights_hidden2_output))\n",
    "        return self.output\n",
    "\n",
    "    def backpropagation(self, features, labels, learning_rate):\n",
    "        #so labels has the right shape for opperations\n",
    "        labels = labels.reshape(-1, 1)\n",
    "\n",
    "        self.forward_pass(features)\n",
    "        \n",
    "        \n",
    "        # Backward pass\n",
    "        output_error = labels - self.output\n",
    "\n",
    "         # Calculate deltas starting from output layer\n",
    "        hidden2_error = np.dot(output_error, self.weights_hidden2_output.T)\n",
    "        hidden2_delta = hidden2_error * sigmoid_activation_derivative(self.hidden_input2)\n",
    "\n",
    "        hidden1_error = np.dot(hidden2_delta, self.weights_hidden1_hidden2.T)\n",
    "        hidden1_delta = hidden1_error * sigmoid_activation_derivative(self.hidden_input1)\n",
    "\n",
    "        # # Update weights\n",
    "        # self.weights_hidden2_output +=   np.dot(self.hidden_output2.T, output_error)\n",
    "        # self.weights_hidden1_hidden2 += np.dot(self.hidden_output1.T, hidden2_delta)\n",
    "        # self.weights_input_hidden1 +=  np.dot(features.T, hidden1_delta)\n",
    "\n",
    "        # Update biases\n",
    "        self.bias_hidden2 += hidden2_delta.sum(axis=0)  # Update bias for hidden layer 2\n",
    "        self.bias_hidden1 += hidden1_delta.sum(axis=0)  # Update bias for hidden layer 1\n",
    "\n",
    "        # Update weights having the learning rate there for second question part but will be 1 for first question\n",
    "        self.weights_hidden2_output += learning_rate * np.dot(self.hidden_output2.T, output_error)\n",
    "        self.weights_hidden1_hidden2 += learning_rate * np.dot(self.hidden_output1.T, hidden2_delta)\n",
    "        self.weights_input_hidden1 += learning_rate * np.dot(features.T, hidden1_delta)\n",
    "        return self.weights_hidden2_output, self.weights_hidden1_hidden2, self.weights_input_hidden1, self.bias_hidden2,  self.bias_hidden1\n",
    "\n",
    "\n",
    "    def stochastic_gradient_descent(self, x_train, y_train, x_test, y_test, width, learning_rate, d, epochs, zero_weights =False ):\n",
    "        # Initialize weights with random values for the specified width\n",
    "        if zero_weights == False: \n",
    "            self.weights_input_hidden1 = np.random.randn(self.input_size, width)\n",
    "            self.weights_hidden1_hidden2 = np.random.randn(width, width)\n",
    "            self.weights_hidden2_output = np.random.randn(width, self.output_size)\n",
    "        else:\n",
    "            #for problem 3\n",
    "            self.weights_input_hidden1 = np.zeros((self.input_size, width))\n",
    "            self.weights_hidden1_hidden2 = np.zeros((width, width))\n",
    "            self.weights_hidden2_output = np.zeros((width, self.output_size))\n",
    "        # Initialize biases\n",
    "        self.bias_hidden1 = np.ones((1, width))\n",
    "        self.bias_hidden2 = np.ones((1, width))\n",
    "\n",
    "        # Variables to track objective function values during training\n",
    "        train_err = []\n",
    "        test_err = []\n",
    "        train_accuracy = []\n",
    "        test_accuracy = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "           #shuffle time\n",
    "            shuffled_idx = np.random.permutation(len(x_train))\n",
    "            x_train_shuffled = x_train[shuffled_idx]\n",
    "            y_train_shuffled = y_train[shuffled_idx]\n",
    "\n",
    "         \n",
    "            LR = learning_rate / (1 + (learning_rate / d) * epoch)\n",
    "\n",
    "            # Stochastic Gradient Descent\n",
    "            for i in range(len(x_train_shuffled)):\n",
    "                sample_feature = x_train_shuffled[i].reshape(1, -1)\n",
    "                sample_label = y_train_shuffled[i].reshape(-1, 1)\n",
    "\n",
    "               \n",
    "                self.backpropagation(sample_feature, sample_label, LR)\n",
    "\n",
    "      \n",
    "        train_output = self.forward_pass(x_train)\n",
    "        train_loss = np.mean(np.square(y_train - train_output))\n",
    "\n",
    "        test_output = self.forward_pass(x_test)\n",
    "        test_loss = np.mean(np.square(y_test - test_output))\n",
    "            \n",
    "        train_err.append(train_loss)\n",
    "        test_err.append(test_loss)\n",
    "        return train_err, test_err\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T03:48:25.226463700Z",
     "start_time": "2023-12-09T03:48:25.196317800Z"
    }
   },
   "id": "24eac5eda38152fd"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Gradients: [[-0.4129254 ]\n",
      " [-0.49615295]\n",
      " [ 0.22821415]] [[-2.07411556  0.74958547  0.59384841]\n",
      " [-0.47715872  0.1859279   0.65521873]\n",
      " [-0.85218659 -0.3995431  -0.36171508]] [[ 0.01321342  0.24258624  1.33412005]\n",
      " [-1.6529981   0.88631881 -0.6636908 ]\n",
      " [-0.32629197  1.00022033 -1.07157695]\n",
      " [ 0.27040572  1.05403659 -0.934335  ]]\n",
      "Bias: [[0.87305472 0.74164657 0.91161177]] [[1.         0.99257931 1.0002443 ]]\n"
     ]
    }
   ],
   "source": [
    "# Want to compute the gradeint with respect to one training example\n",
    "input_size = 4\n",
    "hidden_size1 = 3\n",
    "hidden_size2 = 3\n",
    "output_size = 1\n",
    "\n",
    "x_single_sample = x_train[0].reshape(1, -1) \n",
    "y_single_sample = y_train[0].reshape(-1, 1)\n",
    "\n",
    "one_sample_network = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "weights_hidden2_output, weights_hidden1_hidden2, weights_input_hidden1, bias_hidden2, bias_hidden1 = one_sample_network.backpropagation(x_single_sample, y_single_sample, 1)\n",
    "\n",
    "print(\"Weight Gradients:\", weights_hidden2_output, weights_hidden1_hidden2,weights_input_hidden1)\n",
    "print(\"Bias:\", bias_hidden2, bias_hidden1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:39:45.229917200Z",
     "start_time": "2023-12-08T05:39:45.213054Z"
    }
   },
   "id": "ef6fac5238413073"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baile\\AppData\\Local\\Temp\\ipykernel_2332\\3929561123.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Configuration - Gamma: 0.001, d: 0.01, Width: 5\n",
      "Best Training Error: 0.2902630334632157\n",
      "Best Test Error: 0.2871435392484001\n"
     ]
    }
   ],
   "source": [
    "#little experiement to find the best d and LR\n",
    "\n",
    "LR_values = [0.1, 0.01, 0.001]\n",
    "d_values = [0.01, 0.01, 100]\n",
    "width_values = [5, 10, 25, 50, 100]\n",
    "epochs = 100\n",
    "input_size = 4\n",
    "hidden_size1 = 3\n",
    "hidden_size2 = 3\n",
    "output_size = 1\n",
    "\n",
    "best_train_err = float('inf')\n",
    "best_test_err = float('inf')\n",
    "best_learning_rate = None\n",
    "best_d = None\n",
    "best_width = None\n",
    "\n",
    "for gamma in LR_values:\n",
    "    for d_val in d_values:\n",
    "        for width_val in width_values:\n",
    "            \n",
    "            nn = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "            #do gradient descent\n",
    "            train_err, test_err = nn.stochastic_gradient_descent(x_train, y_train, x_test, y_test, width_val, gamma, d_val, epochs)\n",
    "\n",
    "            #now see whihc has the lowest err\n",
    "            if train_err[-1] < best_train_err and test_err[-1] < best_test_err:\n",
    "                best_train_err = train_err[-1]\n",
    "                best_test_err = test_err[-1]\n",
    "                best_learning_rate = gamma\n",
    "                best_d = d_val\n",
    "                best_width = width_val\n",
    "\n",
    "# Output the best ones found\n",
    "print(f\"Best Configuration - Gamma: {best_learning_rate}, d: {best_d}, Width: {best_width}\")\n",
    "print(f\"Best Training Error: {best_train_err}\")\n",
    "print(f\"Best Test Error: {best_test_err}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:42:46.129399900Z",
     "start_time": "2023-12-08T05:39:45.933224900Z"
    }
   },
   "id": "1eafa247811c22cd"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR Width: 5, Training err: [0.2949359456583213], Test err: [0.29445387428394526]\n",
      "ERR Width: 10, Training err: [0.4166329963106501], Test err: [0.410998943323258]\n",
      "ERR Width: 25, Training err: [0.520486313861385], Test err: [0.5012146017915575]\n",
      "ERR Width: 50, Training err: [0.5873224199956868], Test err: [0.5726848990407992]\n",
      "ERR Width: 100, Training err: [0.5838476187837106], Test err: [0.6396642814391519]\n"
     ]
    }
   ],
   "source": [
    "#part b\n",
    "\n",
    "widths = [5,10,25,50,100]\n",
    "d = 0.01\n",
    "learning_rate = 0.001\n",
    "input_size = 4\n",
    "hidden_size1 = 3\n",
    "hidden_size2 = 3\n",
    "output_size = 1\n",
    "epochs = 100\n",
    "\n",
    "for width in widths:\n",
    "    network =  NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    train_err, test_err = network.stochastic_gradient_descent(x_train, y_train, x_test, y_test, width, learning_rate, d, epochs)\n",
    "    print(\n",
    "        f\"ERR Width: {width}, Training err: {train_err}, Test err: {test_err}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T03:47:41.409087800Z",
     "start_time": "2023-12-09T03:47:12.470124600Z"
    }
   },
   "id": "f731fe8d74c5ce6a"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 5, Zero-Initialized Weights, Training Accuracy: [0.24709490626882546], Test Accuracy: [0.2466517593255136]\n",
      "Width: 10, Zero-Initialized Weights, Training Accuracy: [0.2470948919353979], Test Accuracy: [0.24665327268552806]\n",
      "Width: 25, Zero-Initialized Weights, Training Accuracy: [0.24709488972901356], Test Accuracy: [0.2466525819683823]\n",
      "Width: 50, Zero-Initialized Weights, Training Accuracy: [0.24709489002089896], Test Accuracy: [0.24665254383559532]\n",
      "Width: 100, Zero-Initialized Weights, Training Accuracy: [0.24709488984537384], Test Accuracy: [0.24665256605547442]\n"
     ]
    }
   ],
   "source": [
    "for width in widths:\n",
    "    network =  NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    train_err, test_err = network.stochastic_gradient_descent(x_train, y_train, x_test, y_test, width, learning_rate, d, epochs, zero_weights=True)\n",
    "    print(\n",
    "        f\"Width: {width}, Zero-Initialized Weights, Training Err: {train_err}, Test Err: {test_err}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T03:49:03.144307200Z",
     "start_time": "2023-12-09T03:48:31.597972100Z"
    }
   },
   "id": "307788b8d8b091fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TORCH TIME"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16839737a6ece63e"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 3, Width: 5, Activation: tanh, Train Accuracy: 99.20%, Test Accuracy: 98.60%\n",
      "Depth: 3, Width: 5, Activation: relu, Train Accuracy: 73.94%, Test Accuracy: 69.54%\n",
      "Depth: 3, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 10, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 3, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 5, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 5, Activation: relu, Train Accuracy: 99.20%, Test Accuracy: 98.60%\n",
      "Depth: 5, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 10, Activation: relu, Train Accuracy: 99.89%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 99.80%\n",
      "Depth: 5, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 5, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 5, Activation: tanh, Train Accuracy: 99.66%, Test Accuracy: 99.40%\n",
      "Depth: 9, Width: 5, Activation: relu, Train Accuracy: 97.70%, Test Accuracy: 97.39%\n",
      "Depth: 9, Width: 10, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 99.80%\n",
      "Depth: 9, Width: 10, Activation: relu, Train Accuracy: 98.97%, Test Accuracy: 99.40%\n",
      "Depth: 9, Width: 25, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 25, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 50, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 50, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 100, Activation: tanh, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n",
      "Depth: 9, Width: 100, Activation: relu, Train Accuracy: 100.00%, Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "#loading in data for pytorch use\n",
    "def create_dataloader(csv_file, batch_size=32):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    features = torch.tensor(data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "    labels = torch.tensor(data.iloc[:, -1].values, dtype=torch.int64)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "train_loader = create_dataloader('bank-note/train.csv')\n",
    "test_loader = create_dataloader('bank-note/test.csv')\n",
    "\n",
    "class PTNeuralNetwork(nn.Module):\n",
    "    #constructor\n",
    "    def __init__(self, input_size, output_size, hidden_layers, activation_fn):\n",
    "        super(PTNeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_layers)):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_size, hidden_layers[i]))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    #forward Pass method\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#wanna see the value that the model is correct\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def train_and_evaluate(depths, widths, activation_fns, init_methods, train_loader, test_loader):\n",
    "    input_size = 4  \n",
    "    output_size = 2  \n",
    "    num_epochs = 20  \n",
    "\n",
    "    for depth in depths:\n",
    "        for width in widths:\n",
    "            for activation_fn, init_method in zip(activation_fns, init_methods):\n",
    "                hidden_layers = [width] * depth\n",
    "                model = PTNeuralNetwork(input_size, output_size, hidden_layers, activation_fn)\n",
    "                for layer in model.layers:\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        init_method(layer.weight)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    #need to tell the model it is training\n",
    "                    model.train()  \n",
    "                    for data, label in train_loader:\n",
    "                        #WAY EASIER WAY TO DO GRADIENTS \n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(data)\n",
    "                        loss = criterion(output, label)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                train_accuracy = evaluate_accuracy(model, train_loader)\n",
    "                test_accuracy = evaluate_accuracy(model, test_loader)\n",
    "                print(f\"Depth: {depth}, Width: {width}, Activation: {activation_fn.__name__}, \"\n",
    "                      f\"Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Activation functions and initialization methods\n",
    "activation_fns = [torch.tanh, torch.relu]\n",
    "init_methods = [nn.init.xavier_normal_, nn.init.kaiming_normal_]\n",
    "\n",
    "train_and_evaluate([3, 5, 9], [5, 10, 25, 50, 100], activation_fns, init_methods, train_loader, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:45:31.153657600Z",
     "start_time": "2023-12-08T05:44:48.111206300Z"
    }
   },
   "id": "ed1626f6848c5c1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99ea3b0429b429f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
