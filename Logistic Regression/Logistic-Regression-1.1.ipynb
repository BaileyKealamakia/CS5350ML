{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# loading in the data\n",
    "\n",
    "columns = [\n",
    "    'variance',\n",
    "    'skewness',\n",
    "    'curtosis',\n",
    "    'entropy',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "train_data = pd.read_csv('bank-note/train.csv', names=columns, header=None)\n",
    "test_data = pd.read_csv('bank-note/test.csv', names=columns, header=None)\n",
    "\n",
    "\n",
    "x_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "x_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T04:17:01.728085400Z",
     "start_time": "2023-12-09T04:17:01.575861800Z"
    }
   },
   "id": "8823a8cc8dd0a617"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T04:17:02.006445400Z",
     "start_time": "2023-12-09T04:17:01.993534300Z"
    }
   },
   "id": "315c7d18cd950517"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baile\\AppData\\Local\\Temp\\ipykernel_15284\\1445666042.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "C:\\Users\\Baile\\AppData\\Local\\Temp\\ipykernel_15284\\1445666042.py:22: RuntimeWarning: overflow encountered in divide\n",
      "  grad_prior = w / v\n",
      "C:\\Users\\Baile\\AppData\\Local\\Temp\\ipykernel_15284\\1445666042.py:62: RuntimeWarning: invalid value encountered in subtract\n",
      "  w -= gam * grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For v=0.01, Average Train Error = 0.4461009174311928, Average Test Error = 0.4420000000000001\n",
      "For v=0.1, Average Train Error = 0.39784403669724777, Average Test Error = 0.39292\n",
      "For v=0.5, Average Train Error = 0.2783142201834862, Average Test Error = 0.28184\n",
      "For v=1, Average Train Error = 0.2243348623853211, Average Test Error = 0.22932000000000002\n",
      "For v=3, Average Train Error = 0.15606651376146785, Average Test Error = 0.16440000000000002\n",
      "For v=5, Average Train Error = 0.13153669724770642, Average Test Error = 0.14134\n",
      "For v=10, Average Train Error = 0.1010435779816514, Average Test Error = 0.11305999999999998\n",
      "For v=100, Average Train Error = 0.06827981651376146, Average Test Error = 0.07833999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the objective function for logistic regression with Gaussian prior\n",
    "def objective_function(X, y, w, v):\n",
    "    n = len(y)\n",
    "    y_pred = sigmoid(X.dot(w))\n",
    "    likelihood = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) / n\n",
    "    prior = np.sum(w ** 2) / (2 * v)\n",
    "    return likelihood + prior\n",
    "\n",
    "#gradient of the odjective\n",
    "def gradient(X, y, w, v):\n",
    "    n = 1\n",
    "    y_pred = sigmoid(X.dot(w))\n",
    "    grad_likelihood = X.T.dot(y_pred - y) / n\n",
    "    grad_prior = w / v\n",
    "    return grad_likelihood + grad_prior\n",
    "\n",
    "\n",
    "\n",
    "T = 100 \n",
    "learning_rate = 0.1\n",
    "d = 100 \n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "prior_variances = [0.01, 0.1, 0.5, 1, 3, 5, 10, 100]\n",
    "\n",
    "for v in prior_variances:\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    #initialize weights\n",
    "    w = np.zeros(x_train.shape[1])  \n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for epoch in range(T):\n",
    "        \n",
    "        # Shuffle time\n",
    "        indices = np.random.permutation(len(y_train))\n",
    "        X_train_shuffled = x_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        gam = learning_rate / (1 + (learning_rate / d) * epoch)\n",
    "        \n",
    "        #stochastic gradient descent\n",
    "        for i in range(len(y_train)):\n",
    "            grad = gradient(X_train_shuffled[i], y_train_shuffled[i], w, v)\n",
    "            w -= gam * grad\n",
    "\n",
    "       #calculating error and acruacy\n",
    "        train_pred = sigmoid(x_train.dot(w))\n",
    "        train_pred_labels = (train_pred >= 0.5).astype(int)\n",
    "        train_accuracy = np.mean(train_pred_labels == y_train)\n",
    "        train_errors.append(1 - train_accuracy)  \n",
    "        \n",
    "\n",
    "        # Calculate test accuracy\n",
    "        test_pred = sigmoid(x_test.dot(w))\n",
    "        test_pred_labels = (test_pred >= 0.5).astype(int)\n",
    "        test_accuracy = np.mean(test_pred_labels == y_test)\n",
    "        test_errors.append(1 - test_accuracy)\n",
    "\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_test_error = np.mean(test_errors)\n",
    "\n",
    "    print(f\"For v={v}, Average Train Error = {avg_train_error}, Average Test Error = {avg_test_error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T04:19:09.519500700Z",
     "start_time": "2023-12-09T04:19:03.102143500Z"
    }
   },
   "id": "a64bcd22ef4f4ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "69711e5376c7b1ee"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For For ML estimation v=0.01, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=0.1, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=0.5, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=1, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=3, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=5, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=10, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n",
      "For For ML estimation v=100, Average Train Error = 0.053474770642201845, Average Test Error = 0.06404\n"
     ]
    }
   ],
   "source": [
    "# Remove the prior-related parts from the objective function\n",
    "def objective_function(X, y, w):\n",
    "    n = len(y)\n",
    "    y_pred = sigmoid(X.dot(w))\n",
    "    likelihood = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) / n\n",
    "    return likelihood\n",
    "\n",
    "\n",
    "def gradient(X, y, w):\n",
    "    n = 1\n",
    "    y_pred = sigmoid(X.dot(w))\n",
    "    grad_likelihood = X.T.dot(y_pred - y) / n\n",
    "    return grad_likelihood\n",
    "\n",
    "# ML estimation doesn't use prior variance\n",
    "for v in prior_variances:#[0]:\n",
    "    np.random.seed(42)\n",
    "    w = np.zeros(x_train.shape[1])  # Initialize model parameters\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    #basically the same as before\n",
    "    for epoch in range(T):\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(len(y_train))\n",
    "        X_train_shuffled = x_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "\n",
    "        # Update learning rate for this epoch\n",
    "        gamma_t = learning_rate / (1 + (learning_rate / d) * epoch)\n",
    "\n",
    "        # Perform stochastic gradient descent for ML estimation\n",
    "        for i in range(len(y_train)):\n",
    "            grad = gradient(X_train_shuffled[i], y_train_shuffled[i], w)\n",
    "            w -= gamma_t * grad\n",
    "\n",
    "        # Calculate ERR\n",
    "        train_pred = sigmoid(x_train.dot(w))\n",
    "        train_pred_labels = (train_pred >= 0.5).astype(int)\n",
    "        train_accuracy = np.mean(train_pred_labels == y_train)\n",
    "        train_errors.append(1 - train_accuracy)\n",
    "\n",
    "        test_pred = sigmoid(x_test.dot(w))\n",
    "        test_pred_labels = (test_pred >= 0.5).astype(int)\n",
    "        test_accuracy = np.mean(test_pred_labels == y_test)\n",
    "        test_errors.append(1 - test_accuracy)\n",
    "\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_test_error = np.mean(test_errors)\n",
    "    print(f\"For For ML estimation v={v}, Average Train Error = {avg_train_error}, Average Test Error = {avg_test_error}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T04:24:02.128067300Z",
     "start_time": "2023-12-09T04:23:56.980807100Z"
    }
   },
   "id": "e1c923a22776c758"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd36c1b8d01de341"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
